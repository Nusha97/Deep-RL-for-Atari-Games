{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKziZoOKiTBw",
        "colab_type": "text"
      },
      "source": [
        "**OpenAI baseline code for DQN to train the network to play Pong and CartPole**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ABLLYsYa03q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/openai/baselines.git\n",
        "%cd baselines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Fb_nfNdwSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==1.13.1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "%cd baselines\n",
        "!pip install -e .\n",
        "!pip install pytest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEB3JXTRsS_I",
        "colab_type": "text"
      },
      "source": [
        "**Training DQN to play Pong using the hyperparameters in the defaults.py file ('/baselines/baselines/deepq/defaults.py')**                                           \n",
        "You will need to create a models directory to save trained models and log folder  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA960BeweQ-F",
        "colab_type": "code",
        "outputId": "e11b741f-4488-4dbc-d693-bbc7dfe980ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python -m baselines.run --seed='1' --alg=deepq --env=PongNoFrameskip-v4 --num_timesteps=1000000 --save_path=/content/baselines/models/brea.pkl --log_path=/content/baselines/logs/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Logging to /content/baselines/logs/\n",
            "env_type: atari\n",
            "Training deepq on atari:PongNoFrameskip-v4 with arguments \n",
            "{'network': 'conv_only', 'lr': 0.01, 'buffer_size': 10000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'train_freq': 4, 'learning_starts': 10000, 'target_network_update_freq': 1000, 'gamma': 0.99, 'prioritized_replay': True, 'prioritized_replay_alpha': 0.6, 'checkpoint_freq': 10000, 'checkpoint_path': None, 'dueling': True}\n",
            "2020-04-15 23:03:20.255466: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-04-15 23:03:20.429393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-15 23:03:20.430139: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2bd4100 executing computations on platform CUDA. Devices:\n",
            "2020-04-15 23:03:20.430171: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-04-15 23:03:20.433255: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000189999 Hz\n",
            "2020-04-15 23:03:20.433694: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2bd29a0 executing computations on platform Host. Devices:\n",
            "2020-04-15 23:03:20.433745: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2020-04-15 23:03:20.433928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 15.90GiB freeMemory: 15.64GiB\n",
            "2020-04-15 23:03:20.433954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2020-04-15 23:03:20.434757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-04-15 23:03:20.434785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2020-04-15 23:03:20.434794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2020-04-15 23:03:20.434915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /content/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2020-04-15 23:03:21.910091: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 13       |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | -20.4    |\n",
            "| steps                   | 8.71e+04 |\n",
            "--------------------------------------\n",
            "Saving model due to mean reward increase: None -> -20.5\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 1.66e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | -21      |\n",
            "| steps                   | 2.45e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 3.24e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 4.03e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 4.82e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 5.61e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 6.39e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 7.18e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 1e+03    |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 7.97e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 1.1e+03  |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 8.76e+05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 1        |\n",
            "| episodes                | 1.2e+03  |\n",
            "| mean 100 episode reward | -20.9    |\n",
            "| steps                   | 9.54e+05 |\n",
            "--------------------------------------\n",
            "Restored model with mean reward: -20.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF27ZXquyfG5",
        "colab_type": "text"
      },
      "source": [
        "**Training DQN to play CartPole**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcU8_2kAzAZG",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameter Tuning** **for** **CartPole**                                              \n",
        "The defaults.py file contains the hyperparameters used for training DQN('/baselines/baselines/deepq/defaults.py')                                                              \n",
        "**Trial 1:** Batch size - 32, Learning rate - 0.0001, Timesteps - 1000000   ##Mean reward - 175.4                                    \n",
        "**Trial 2**: Batch size - 256. Learning rate - 0.0005, Timesteps - 1000000 #Mean Reward - 195.2        \n",
        "**Trial 3:** Batch size - 256, Learning rate - 0.001 - ##Mean reward - 192.8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrHDuQY_ycgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m baselines.run --alg=deepq --env=CartPole-v0 --save_path=/content/baselines/models/cart.pkl --num_timesteps=100000 --log_path=/content/baselines/logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRF_T88ww0OB",
        "colab_type": "text"
      },
      "source": [
        "**Testing the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo2dyVgfLanB",
        "colab_type": "code",
        "outputId": "0927c7bd-35f6-4b0a-8484-38f63dfbe2d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "import sys\n",
        "import re\n",
        "import multiprocessing\n",
        "import os.path as osp\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from baselines.common.vec_env import VecFrameStack, VecNormalize, VecEnv\n",
        "from baselines.common.vec_env.vec_video_recorder import VecVideoRecorder\n",
        "from baselines.common.cmd_util import common_arg_parser, parse_unknown_args, make_vec_env, make_env\n",
        "from baselines.common.tf_util import get_session\n",
        "from baselines import logger\n",
        "from importlib import import_module\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(700, 500))\n",
        "display.start()\n",
        "\n",
        "try:\n",
        "    from mpi4py import MPI\n",
        "except ImportError:\n",
        "    MPI = None\n",
        "\n",
        "try:\n",
        "    import pybullet_envs\n",
        "except ImportError:\n",
        "    pybullet_envs = None\n",
        "\n",
        "try:\n",
        "    import roboschool\n",
        "except ImportError:\n",
        "    roboschool = None\n",
        "\n",
        "_game_envs = defaultdict(set)\n",
        "for env in gym.envs.registry.all():\n",
        "    # TODO: solve this with regexes\n",
        "    env_type = env.entry_point.split(':')[0].split('.')[-1]\n",
        "    _game_envs[env_type].add(env.id)\n",
        "\n",
        "# reading benchmark names directly from retro requires\n",
        "# importing retro here, and for some reason that crashes tensorflow\n",
        "# in ubuntu\n",
        "_game_envs['retro'] = {\n",
        "    'BubbleBobble-Nes',\n",
        "    'SuperMarioBros-Nes',\n",
        "    'TwinBee3PokoPokoDaimaou-Nes',\n",
        "    'SpaceHarrier-Nes',\n",
        "    'SonicTheHedgehog-Genesis',\n",
        "    'Vectorman-Genesis',\n",
        "    'FinalFight-Snes',\n",
        "    'SpaceInvaders-Snes',\n",
        "}\n",
        "\n",
        "\n",
        "def train(args, extra_args):\n",
        "    env_type, env_id = get_env_type(args)\n",
        "    print('env_type: {}'.format(env_type))\n",
        "\n",
        "    total_timesteps = int(args.num_timesteps)\n",
        "    seed = args.seed\n",
        "\n",
        "    learn = get_learn_function(args.alg)\n",
        "    alg_kwargs = get_learn_function_defaults(args.alg, env_type)\n",
        "    alg_kwargs.update(extra_args)\n",
        "\n",
        "    env = build_env(args)\n",
        "    if args.save_video_interval != 0:\n",
        "        env = VecVideoRecorder(env, osp.join(logger.get_dir(), \"videos\"), record_video_trigger=lambda x: x % args.save_video_interval == 0, video_length=args.save_video_length)\n",
        "\n",
        "    if args.network:\n",
        "        alg_kwargs['network'] = args.network\n",
        "    else:\n",
        "        if alg_kwargs.get('network') is None:\n",
        "            alg_kwargs['network'] = get_default_network(env_type)\n",
        "\n",
        "    print('Training {} on {}:{} with arguments \\n{}'.format(args.alg, env_type, env_id, alg_kwargs))\n",
        "\n",
        "    model = learn(\n",
        "        env=env,\n",
        "        seed=seed,\n",
        "        total_timesteps=total_timesteps,\n",
        "        **alg_kwargs\n",
        "    )\n",
        "\n",
        "    return model, env\n",
        "\n",
        "\n",
        "def build_env(args):\n",
        "    ncpu = multiprocessing.cpu_count()\n",
        "    if sys.platform == 'darwin': ncpu //= 2\n",
        "    nenv = args.num_env or ncpu\n",
        "    alg = args.alg\n",
        "    seed = args.seed\n",
        "\n",
        "    env_type, env_id = get_env_type(args)\n",
        "\n",
        "    if env_type in {'atari', 'retro'}:\n",
        "        if alg == 'deepq':\n",
        "            env = make_env(env_id, env_type, seed=seed, wrapper_kwargs={'frame_stack': True})\n",
        "        elif alg == 'trpo_mpi':\n",
        "            env = make_env(env_id, env_type, seed=seed)\n",
        "        else:\n",
        "            frame_stack_size = 4\n",
        "            env = make_vec_env(env_id, env_type, nenv, seed, gamestate=args.gamestate, reward_scale=args.reward_scale)\n",
        "            env = VecFrameStack(env, frame_stack_size)\n",
        "\n",
        "    else:\n",
        "        config = tf.ConfigProto(allow_soft_placement=True,\n",
        "                               intra_op_parallelism_threads=1,\n",
        "                               inter_op_parallelism_threads=1)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        get_session(config=config)\n",
        "\n",
        "        flatten_dict_observations = alg not in {'her'}\n",
        "        env = make_vec_env(env_id, env_type, args.num_env or 1, seed, reward_scale=args.reward_scale, flatten_dict_observations=flatten_dict_observations)\n",
        "\n",
        "        if env_type == 'atari':\n",
        "            env = VecNormalize(env, use_tf=True)\n",
        "\n",
        "    return env\n",
        "\n",
        "\n",
        "def get_env_type(args):\n",
        "    env_id = args.env\n",
        "\n",
        "    if args.env_type is not None:\n",
        "        return args.env_type, env_id\n",
        "\n",
        "    # Re-parse the gym registry, since we could have new envs since last time.\n",
        "    for env in gym.envs.registry.all():\n",
        "        env_type = env.entry_point.split(':')[0].split('.')[-1]\n",
        "        _game_envs[env_type].add(env.id)  # This is a set so add is idempotent\n",
        "\n",
        "    if env_id in _game_envs.keys():\n",
        "        env_type = env_id\n",
        "        env_id = [g for g in _game_envs[env_type]][0]\n",
        "    else:\n",
        "        env_type = None\n",
        "        for g, e in _game_envs.items():\n",
        "            if env_id in e:\n",
        "                env_type = g\n",
        "                break\n",
        "        if ':' in env_id:\n",
        "            env_type = re.sub(r':.*', '', env_id)\n",
        "        assert env_type is not None, 'env_id {} is not recognized in env types'.format(env_id, _game_envs.keys())\n",
        "\n",
        "    return env_type, env_id\n",
        "\n",
        "\n",
        "def get_default_network(env_type):\n",
        "    if env_type in {'atari', 'retro'}:\n",
        "        return 'cnn'\n",
        "    else:\n",
        "        return 'mlp'\n",
        "\n",
        "def get_alg_module(alg, submodule=None):\n",
        "    submodule = submodule or alg\n",
        "    try:\n",
        "        # first try to import the alg module from baselines\n",
        "        alg_module = import_module('.'.join(['baselines', alg, submodule]))\n",
        "    except ImportError:\n",
        "        # then from rl_algs\n",
        "        alg_module = import_module('.'.join(['rl_' + 'algs', alg, submodule]))\n",
        "\n",
        "    return alg_module\n",
        "\n",
        "\n",
        "def get_learn_function(alg):\n",
        "    return get_alg_module(alg).learn\n",
        "\n",
        "\n",
        "def get_learn_function_defaults(alg, env_type):\n",
        "    try:\n",
        "        alg_defaults = get_alg_module(alg, 'defaults')\n",
        "        kwargs = getattr(alg_defaults, env_type)()\n",
        "    except (ImportError, AttributeError):\n",
        "        kwargs = {}\n",
        "    return kwargs\n",
        "\n",
        "\n",
        "\n",
        "def parse_cmdline_kwargs(args):\n",
        "    '''\n",
        "    convert a list of '='-spaced command-line arguments to a dictionary, evaluating python objects when possible\n",
        "    '''\n",
        "    def parse(v):\n",
        "\n",
        "        assert isinstance(v, str)\n",
        "        try:\n",
        "            return eval(v)\n",
        "        except (NameError, SyntaxError):\n",
        "            return v\n",
        "\n",
        "    return {k: parse(v) for k,v in parse_unknown_args(args).items()}\n",
        "\n",
        "\n",
        "def configure_logger(log_path, **kwargs):\n",
        "    if log_path is not None:\n",
        "        logger.configure(log_path)\n",
        "    else:\n",
        "        logger.configure(**kwargs)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # configure logger, disable logging in child MPI processes (with rank > 0)\n",
        "    m=0\n",
        "    arg_parser = common_arg_parser()\n",
        "    args, unknown_args = arg_parser.parse_known_args(args)\n",
        "    \n",
        "    extra_args = parse_cmdline_kwargs(unknown_args)\n",
        "\n",
        "    if MPI is None or MPI.COMM_WORLD.Get_rank() == 0:\n",
        "        rank = 0\n",
        "        configure_logger(args.log_path)\n",
        "    else:\n",
        "        rank = MPI.COMM_WORLD.Get_rank()\n",
        "        configure_logger(args.log_path, format_strs=[])\n",
        "\n",
        "    model, env = train(args, extra_args)\n",
        "\n",
        "    if args.save_path is not None and rank == 0:\n",
        "        #save_path = osp.expanduser(args.save_path) #removed\n",
        "        save_path = args.save_path #added\n",
        "        model.save(save_path)\n",
        "\n",
        "    if args.play:\n",
        "      #pl(env,model)\n",
        "        logger.log(\"Running trained model\")\n",
        "        obs = env.reset()\n",
        "\n",
        "        state = model.initial_state if hasattr(model, 'initial_state') else None\n",
        "        dones = np.zeros((1,))\n",
        "\n",
        "        episode_rew = np.zeros(env.num_envs) if isinstance(env, VecEnv) else np.zeros(1)\n",
        "        while True:\n",
        "            if state is not None:\n",
        "                actions, _, state, _ = model.step(obs,S=state, M=dones)\n",
        "            else:\n",
        "                actions, _, _, _ = model.step(obs)\n",
        "\n",
        "            obs, rew, done, _ = env.step(actions)\n",
        "            episode_rew += rew\n",
        "            #env.render()\n",
        "            %matplotlib inline\n",
        "            screen = env.render(mode='rgb_array')       \n",
        "            plt.imshow(screen)\n",
        "            k=str(m)+'image_new.jpg'\n",
        "            plt.imsave('/content/drive/My Drive/im/'+k,screen )\n",
        "            m=m+1\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "            done_any = done.any() if isinstance(done, np.ndarray) else done\n",
        "            if done_any:\n",
        "                for i in np.nonzero(done):\n",
        "                    print('episode_rew={}'.format(episode_rew[i]))\n",
        "                    episode_rew[i] = 0\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(['/content/baselines/baselines/run.py', '--alg=deepq', '--env=PongNoFrameskip-v4', '--num_timesteps=0', '--load_path=/content/baselines/models/brea.pkl', '--play'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a90a890c692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVecFrameStack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecNormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_video_recorder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVecVideoRecorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmd_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_arg_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_unknown_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_vec_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'baselines'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}